---
title: LLM (大语言模型)
date: 2026-03-01T07:06:26.448Z
category: AI 模型
tags:
  - 大模型
  - Transformer
  - 自然语言处理
  - 存算博弈
description: 基于 Transformer 架构的超大规模深度学习模型，推动了认知智能的质变。
---

> [!NOTE]
> 基于 Transformer 架构的超大规模深度学习模型，推动了认知智能的质变。

## 💡 核心解析
LLM 在边缘侧（手机/端侧）的挑战主要在于‘内存墙’。一个 7B 参数的模型即使经过 INT4 量化，也需要占用近 4GB 的显存带宽。推理过程中的 KV Cache 增长更是会导致内存占用呈平方级上升。因此，端侧 LLM 的性能往往不取决于 NPU 算力，而取决于 LPDDR 内存的带宽和延迟。

## 📊 关键指标
- **Tokens/s**:  每一秒模型能生成的词数
- **Prefill Latency**:  首次响应时间
- **Context Window**:  上下文窗口容量

## 🚀 硅基视角
2026 年的趋势是驱动‘模型蒸馏’与‘硬件协同’。当模型知道 NPU 的底层 Cache 长度时，生成的代码效率会提高一个量级。这正是苹果 A 系列芯片保持霸主地位的阳谋。

---
*本条目由 GJNX AI 引擎自动挖掘并生成，旨在构建《硅基能效通识》知识体系。*
