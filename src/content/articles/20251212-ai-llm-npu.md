---
title: "ã€ç¡…åŸºåå¸¸è¯†ã€‘AI ä¸ºä»€ä¹ˆè¯´åˆ°ä¸€åŠä¼šâ€œå¿˜è®°â€ï¼Ÿæ­ç§˜ LLM æ¨ç†ä¸­çš„ NPU å†…å­˜æŠ¢å ä¸ä¸Šä¸‹æ–‡æº¢å‡º"
date: "2025-12-15T00:00:00.000Z"
tags: ["LLMæ¨ç†","NPU","æ˜¾å­˜ç®¡ç†","ä¸Šä¸‹æ–‡çª—å£"]
category: "Deep Column"
description: "ğŸ“„ Abstract\r \r >   æ‘˜è¦ï¼š  \r > ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„åœºæ™¯ï¼šä¸ AI èŠå¾—æ­£å¼€å¿ƒï¼Œå®ƒçªç„¶â€œå¤±å¿†â€äº†ï¼Œç”šè‡³å¼€å§‹èƒ¡è¨€ä¹±è¯­ï¼Ÿè¿™å¹¶éæ¨¡å‹å˜ç¬¨äº†ï¼Œè€Œæ˜¯å®ƒçš„å¤§è„‘ï¼ˆæ˜¾å­˜ï¼‰è¢«å¡æ»¡äº†ã€‚åœ¨ç«¯ä¾§ LLM æ¨ç†ä¸­ï¼Œ  KV Cache   çš„å¢é•¿é€Ÿåº¦è¿œè¶…æƒ³è±¡ã€‚å½“ NPU çš„æ˜¾å­˜æ± è€—å°½æ—¶ï¼Œè°ƒåº¦å™¨ä¼šè§¦å‘   Paged..."
---

### ğŸ“„ Abstract

> **æ‘˜è¦ï¼š**
> ä½ æ˜¯å¦é‡åˆ°è¿‡è¿™æ ·çš„åœºæ™¯ï¼šä¸ AI èŠå¾—æ­£å¼€å¿ƒï¼Œå®ƒçªç„¶â€œå¤±å¿†â€äº†ï¼Œç”šè‡³å¼€å§‹èƒ¡è¨€ä¹±è¯­ï¼Ÿè¿™å¹¶éæ¨¡å‹å˜ç¬¨äº†ï¼Œè€Œæ˜¯å®ƒçš„å¤§è„‘ï¼ˆæ˜¾å­˜ï¼‰è¢«å¡æ»¡äº†ã€‚åœ¨ç«¯ä¾§ LLM æ¨ç†ä¸­ï¼Œ**KV Cache** çš„å¢é•¿é€Ÿåº¦è¿œè¶…æƒ³è±¡ã€‚å½“ NPU çš„æ˜¾å­˜æ± è€—å°½æ—¶ï¼Œè°ƒåº¦å™¨ä¼šè§¦å‘ **PagedAttention çš„â€œé¡µé¢ç½®æ¢â€** æˆ–å¼ºåˆ¶ **Token ä¸¢å¼ƒ**ã€‚æœ¬æ–‡å°†æ­ç¤ºè¿™ç§ç‰©ç†å±‚é¢çš„â€œè®°å¿†æ¸…æ´—â€å¦‚ä½•å¯¼è‡´äº†é€»è¾‘å±‚é¢çš„â€œä¸Šä¸‹æ–‡æº¢å‡ºâ€ã€‚

---

## 1. ğŸ¤¯ å›°å¢ƒï¼šä¸ºä»€ä¹ˆèŠä¹…äº†ï¼ŒAI å°±å˜æˆäº†â€œé‡‘é±¼è®°å¿†â€ï¼Ÿ

2025 å¹´ï¼Œç«¯ä¾§å¤§æ¨¡å‹ï¼ˆå¦‚ Gemini Nano 3.0, Llama 4-Mobileï¼‰å·²ç»æ ‡é…äº† 128K ç”šè‡³ 1M çš„ä¸Šä¸‹æ–‡çª—å£ã€‚ç†è®ºä¸Šï¼Œå®ƒåº”è¯¥è®°å¾—ä½ ä¸‰å¤©å‰è¯´çš„æ¯ä¸€å¥è¯ã€‚
ä½†ç°å®æ˜¯ï¼Œå½“ä½ è¿ç»­å¯¹è¯è¶…è¿‡ 20 è½®ï¼Œæˆ–è€…è®©å®ƒé˜…è¯»ä¸€ä»½ 50MB çš„ PDF åï¼Œå®ƒå¾€å¾€ä¼šå¿˜è®°å¼€å¤´è®¾å®šçš„è§„åˆ™ï¼Œç”šè‡³åœ¨ç”Ÿæˆé•¿æ–‡æœ¬æ—¶çªç„¶ä¸­æ–­ã€‚


è™½ç„¶æ¨¡å‹**æ”¯æŒ** 128K ä¸Šä¸‹æ–‡ï¼Œä½†è¿™åªæ˜¯ç®—æ³•ä¸Šé™ã€‚ç‰©ç†ä¸Šé™ç”±ä½ çš„**æ‰‹æœºå†…å­˜ï¼ˆRAMï¼‰**å†³å®šã€‚
ä¸€ä¸ª 7B æ¨¡å‹ï¼Œæ¯ç”Ÿæˆä¸€ä¸ª Tokenï¼Œéƒ½éœ€è¦æ¶ˆè€—æ˜¾å­˜æ¥å­˜å‚¨ KV Cacheã€‚å½“å¯¹è¯å˜é•¿ï¼ŒKV Cache ä¼šåƒæ»šé›ªçƒä¸€æ ·æŒ‡æ•°çº§åå™¬å†…å­˜ã€‚ä¸€æ—¦è¾¾åˆ°ç³»ç»Ÿè®¾å®šçš„ **Memory Limitï¼ˆå†…å­˜çº¢çº¿ï¼‰**ï¼ŒOS ä¼šæ¯«ä¸ç•™æƒ…åœ°ä»‹å…¥ï¼Œå¼ºåˆ¶å›æ”¶å†…å­˜ã€‚AI çš„â€œå¤±å¿†â€ï¼Œå…¶å®æ˜¯ç³»ç»Ÿä¸ºäº†é˜²æ­¢æ‰‹æœºæ­»æœºè€Œæ‰§è¡Œçš„ **OOM (Out of Memory) ä¿æŠ¤æœºåˆ¶**ã€‚



---

## 2. ğŸŒ¡ï¸ æ ¸å¿ƒåŸç†ï¼šKV Cache çš„çˆ†ç‚¸ä¸æ˜¾å­˜æŠ¢å 

è¦ç†è§£ AI çš„è®°å¿†ï¼Œå¿…é¡»ç†è§£ Transformer çš„æ¨ç†æœºåˆ¶ã€‚AI ä¸æ˜¯æŠŠçœ‹è¿‡çš„ä¹¦èƒŒä¸‹æ¥ï¼Œè€Œæ˜¯æŠŠå½“å‰çš„å¯¹è¯è½¬åŒ–ä¸º **Key-Value (KV) çŸ©é˜µ** å­˜èµ·æ¥ã€‚

### 2.1 KV Cacheï¼šæ˜‚è´µçš„çŸ­æœŸè®°å¿†

å¯¹äºæ¯ä¸€ä¸ªè¾“å…¥çš„ Tokenï¼Œæ¨¡å‹éƒ½ä¼šè®¡ç®—å‡ºä¸€ç»„ Key å’Œ Value å‘é‡ã€‚
$$Memory_{KV} = 2 \times \text{Layers} \times \text{Heads} \times \text{Dimension} \times \text{Precision} \times \text{Context Length}$$

ä»¥ä¸€ä¸ªæ ‡å‡†çš„ 7B æ¨¡å‹ï¼ˆFP16ç²¾åº¦ï¼‰ä¸ºä¾‹ï¼š
* **1k Context:** å ç”¨çº¦ 0.5GB æ˜¾å­˜ã€‚
* **32k Context:** å ç”¨é£™å‡è‡³ **16GB**ã€‚
* **128k Context:** éœ€è¦ **64GB** æ˜¾å­˜ï¼

åœ¨æ‰‹æœºåªæœ‰ 12GB æˆ– 16GB ç»Ÿä¸€å†…å­˜ï¼ˆUnified Memoryï¼‰çš„æƒ…å†µä¸‹ï¼Œç•™ç»™ KV Cache çš„ç©ºé—´æå…¶æœ‰é™ï¼ˆé€šå¸¸åªæœ‰ 2GB - 4GBï¼‰ã€‚

### 2.2 æŠ•æœºé‡‡æ · (Speculative Decoding) çš„ä»£ä»·

ä¸ºäº†è®© AI è¯´è¯æ›´å¿«ï¼ŒNPU é€šå¸¸å¼€å¯ **æŠ•æœºé‡‡æ ·**ï¼šä¸€ä¸ªå°æ¨¡å‹å…ˆå¿«é€ŸçŒœå‡ºåé¢ 5 ä¸ªè¯ï¼Œå¤§æ¨¡å‹å†æ¥éªŒè¯ã€‚
ä½†è¿™éœ€è¦åŒæ—¶ç»´æŠ¤ä¸¤ä¸ªæ¨¡å‹çš„ KV Cacheï¼Œç¬é—´å†…å­˜å‹åŠ›åŠ å€ã€‚
å½“ç³»ç»Ÿæ£€æµ‹åˆ°å†…å­˜åƒç´§ï¼Œå®ƒä¼šç«‹åˆ»æ€æ‰â€œæŠ•æœºâ€çº¿ç¨‹ï¼Œå¯¼è‡´ AI ç”Ÿæˆé€Ÿåº¦çªç„¶å˜æ…¢ï¼ˆå¡é¡¿ï¼‰ï¼Œç”šè‡³å› ä¸ºçŠ¶æ€åŒæ­¥å¤±è´¥è€Œå¯¼è‡´é€»è¾‘æ–­å±‚ï¼ˆå¹»è§‰ï¼‰ã€‚



---

## 3. âš™ï¸ æ ¸å¿ƒæ¶æ„ï¼šPagedAttention ä¸â€œæ»‘åŠ¨çª—å£â€çš„å¦¥å

ä¸ºäº†åœ¨æœ‰é™å†…å­˜é‡Œå¡è¿›æ›´å¤šå¯¹è¯ï¼Œå·¥ç¨‹å¸ˆä»¬å¼•å…¥äº†ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„ **è™šæ‹Ÿå†…å­˜ç®¡ç†æŠ€æœ¯**ã€‚

### 3.1 PagedAttentionï¼šæŠŠè®°å¿†åˆ‡ç¢

ä¼ ç»Ÿçš„ KV Cache å¿…é¡»å ç”¨è¿ç»­çš„æ˜¾å­˜å—ï¼Œå®¹æ˜“é€ æˆ**å†…å­˜ç¢ç‰‡**ï¼ˆæ˜æ˜æœ‰ç©ºä½™å†…å­˜ï¼Œä½†å¡ä¸è¿›å»ï¼‰ã€‚
**PagedAttention** æŠ€æœ¯å°† KV Cache åˆ‡åˆ†æˆä¸€ä¸ªä¸ªå°çš„ **Blockï¼ˆé¡µé¢ï¼‰**ï¼Œåˆ†æ•£å­˜å‚¨åœ¨ç‰©ç†å†…å­˜çš„å„ä¸ªè§’è½ã€‚
* **ä¼˜åŠ¿ï¼š** å†…å­˜åˆ©ç”¨ç‡æ¥è¿‘ 100%ã€‚
* **é£é™©ï¼š** å½“æ‰€æœ‰ Block éƒ½ç”¨å®Œæ—¶ï¼Œå¿…é¡»è¿›è¡Œ **Swap (æ¢é¡µ)**ã€‚å¦‚æœæ‰‹æœºçš„é—ªå­˜ï¼ˆSwap åˆ†åŒºï¼‰è¯»å†™é€Ÿåº¦è·Ÿä¸ä¸Š NPU çš„è®¡ç®—é€Ÿåº¦ï¼ŒAI å°±ä¼šåƒæ–­ç”µä¸€æ ·çªç„¶å¡ä½ã€‚

### 3.2 æ»‘åŠ¨çª—å£ (Sliding Window) ä¸é©±é€ç­–ç•¥

å½“æ˜¾å­˜å½»åº•è€—å°½æ—¶ï¼Œç³»ç»Ÿå¿…é¡»æ‰§è¡Œ **é©±é€ï¼ˆEvictionï¼‰** ç­–ç•¥ï¼š
1.  **FIFO (å…ˆè¿›å…ˆå‡º):** æ‰”æ‰æœ€æ—©çš„å¯¹è¯ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ AI è®°å¾—ä½ åˆšæ‰è¯´çš„è¯ï¼Œå´å¿˜äº†å¼€å¤´è®¾å®šçš„â€œä½ æ˜¯ä¸€ä¸ªç‰©ç†å­¦å®¶â€ã€‚
2.  **Attention Sink (æ³¨æ„åŠ›æ±‡èšç‚¹):** è¿™æ˜¯ä¸€ä¸ªåç›´è§‰çš„å‘ç°ã€‚åªè¦ä¿ç•™å¼€å¤´çš„å‰å‡ ä¸ª Tokenï¼ˆé€šå¸¸æ˜¯ Promptï¼‰å’Œæœ€è¿‘çš„ Tokenï¼ŒAI å°±èƒ½ä¿æŒåŸºæœ¬çš„é€»è¾‘è¿è´¯æ€§ã€‚ç³»ç»Ÿä¼šâ€œæ‰‹æœ¯åˆ€å¼â€åœ°åˆ‡é™¤ä¸­é—´çš„å¯¹è¯è®°å½•ã€‚


å½“ä¸­é—´çš„å…³é”®ä¿¡æ¯ï¼ˆæ¯”å¦‚â€œæˆ‘ä¸åƒé¦™èœâ€ï¼‰è¢«é©±é€åï¼ŒAI åœ¨æ£€ç´¢è®°å¿†æ—¶å‘ç°è¿™éƒ¨åˆ†æ˜¯ç©ºçš„ï¼Œäºæ˜¯å®ƒå¼€å§‹åŸºäºæ¦‚ç‡è¿›è¡Œ **â€œè„‘è¡¥â€**ã€‚è¿™å°±æ˜¯ç”¨æˆ·çœ‹åˆ°çš„â€œä¸€æœ¬æ­£ç»èƒ¡è¯´å…«é“â€ã€‚



---

## 4. ğŸŒ è¡Œä¸šå±•æœ›ï¼šç«¯ä¾§ AI çš„â€œè®°å¿†é©å‘½â€

ä¸ºäº†è§£å†³â€œé‡‘é±¼è®°å¿†â€ï¼Œè¡Œä¸šæ­£åœ¨æ¢ç´¢æ–°çš„æ–¹å‘ã€‚

* **æ— é™ä¸Šä¸‹æ–‡æ¶æ„ (Infini-attention):** Google å’Œ DeepMind æ­£åœ¨ç ”ç©¶å°†é•¿æœŸè®°å¿†å‹ç¼©æˆä¸€ç§ **â€œç¥ç»å­˜å‚¨å‘é‡â€**ï¼Œè€Œä¸æ˜¯åŸå§‹çš„ KV Cacheã€‚è¿™æ ·ï¼Œå‡ æœ¬ä¹¦çš„å†…å®¹å¯ä»¥è¢«å‹ç¼©åˆ°å‡  MB çš„ç©ºé—´é‡Œã€‚
* **æ··åˆå­˜å‚¨ (Tiered Memory):** åˆ©ç”¨ CXL æŠ€æœ¯æˆ–è¶…é«˜é€Ÿ SSDï¼Œæ„å»º **DRAM-Flash æ··åˆå¯»å€**ã€‚å…è®¸ KV Cache æº¢å‡ºåˆ°é—ªå­˜ä¸­ï¼Œè™½ç„¶é€Ÿåº¦ç¨æ…¢ï¼Œä½†å®¹é‡å‡ ä¹æ— é™ã€‚

## 5. ğŸ† æ€»ç»“ä¸äº’åŠ¨ï¼šç‰©ç†ç©ºé—´å†³å®šæ€ç»´å¹¿åº¦

### 5.1 æœ€ç»ˆç»“è®º (Final Thesis)

AI çš„â€œé—å¿˜â€ä¸æ˜¯è½¯ä»¶ Bugï¼Œè€Œæ˜¯ **ç‰©ç†å†…å­˜ç©ºé—´çš„åˆšæ€§çº¦æŸ**ã€‚åªè¦ DRAM çš„å®¹é‡å’Œå¸¦å®½æ²¡æœ‰è´¨çš„é£è·ƒï¼Œç«¯ä¾§å¤§æ¨¡å‹çš„é•¿çª—å£ä½“éªŒæ°¸è¿œæ˜¯åœ¨â€œä¸¢åŒ…â€å’Œâ€œå‹ç¼©â€ä¸­å¯»æ‰¾å¹³è¡¡ã€‚**å†…å­˜çš„å¤§å°ï¼Œå†³å®šäº† AI çµé­‚çš„åšåº¦ã€‚**

### 5.2 ã€ç¡…åŸºé—®ç­”ã€‘ 

ä½ æ„¿æ„ä¸ºâ€œä¸é—å¿˜çš„ AIâ€ä»˜å‡ºä»€ä¹ˆä»£ä»·ï¼Ÿ

> **è¯·åœ¨è¯„è®ºåŒºæŠ•ç¥¨ï¼š**
> * **A. åŠ é’±å…šï¼š** ä¸‹å°æ‰‹æœºæˆ‘ä¸€å®šä¹° 24GB ç”šè‡³ 32GB å†…å­˜ç‰ˆæœ¬ï¼Œå“ªæ€•è´µ 1000 å—ï¼Œä¹Ÿè¦è®© AI æ»¡è¡€è®°å¿†ã€‚
> * **B. äº‘ç«¯å…šï¼š** æ‰‹æœºå­˜ä¸ä¸‹å°±ä¼ åˆ°äº‘ç«¯ç®—å§ï¼Œæˆ‘ä¸åœ¨ä¹éšç§ï¼Œåªåœ¨ä¹å®ƒåˆ«å¿˜äº†æˆ‘è¯´è¿‡å•¥ã€‚

---

### ğŸ“š å‚è€ƒæ–‡çŒ® / References

1.  **[vLLM Team Research]** *"PagedAttention: Efficient Memory Management for Large Language Model Serving with KV Cache Sharing."* (æ³¨ï¼šå…³äº PagedAttention æŠ€æœ¯çš„å¼€å±±ä¹‹ä½œ)
2.  **[Qualcomm AI Research]** *"Quantizing KV Cache for Efficient LLM Inference on Mobile Devices."* (æ³¨ï¼šå…³äºç§»åŠ¨ç«¯ KV Cache é‡åŒ–å‹ç¼©çš„å·¥ç¨‹å®è·µ)
3.  **[MIT CSAIL]** *"StreamingLLM: Efficient Streaming Language Models with Attention Sinks."* (æ³¨ï¼šå…³äºæ»‘åŠ¨çª—å£å’Œæ³¨æ„åŠ›æ±‡èšç‚¹çš„ç†è®ºåŸºç¡€)